---
title: "Q5 Cleaned"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(glmnet)
```

```{r}
test.documents = read.csv("../data/ReutersC50/test_documents3.csv", header = TRUE, row.names = 1)
test.bigrams = read.csv("../data/ReutersC50/test_bigrams3.csv", header = TRUE, row.names = 1)

train.documents = read.csv("../data/ReutersC50/train_documents3.csv", header = TRUE, row.names = 1)
train.bigrams = read.csv("../data/ReutersC50/train_bigrams3.csv", header = TRUE, row.names = 1)


train.bigrams = train.bigrams + .001
test.bigrams = test.bigrams + .001
```



Start here:

```{r}
N = nrow(train.bigrams)
D = ncol(train.bigrams)

# TF weights
TF_mat = train.bigrams/rowSums(train.bigrams)
# IDF weights
IDF_vec = log(1 + N/colSums(train.bigrams > 0))
# TF-IDF weights:
# use sweep to multiply the columns (margin = 2) by the IDF weights
TFIDF_mat = sweep(TF_mat, MARGIN=2, STATS=IDF_vec, FUN="*")
```

```{r}
# PCA on the TF-IDF weights
pca = prcomp(TFIDF_mat, scale=TRUE)
pve = summary(pca)$importance[3,]
plot(pve)  # not much of an elbow
```

```{r}
X = pca$x[,1:1000]
y = {train.documents$author}
```





Now do the PCA for test set and predict:

```{r}
N = nrow(test.bigrams)
D = ncol(test.bigrams)

#remove 0 row sums
test.bigrams = test.bigrams[rowSums(test.bigrams) != 0,]
test.documents = test.documents[row.names(test.bigrams),]
#remove 0 col sums
test.bigrams = test.bigrams[,colSums(test.bigrams) != 0]

# TF weights
TF_mat = test.bigrams/rowSums(test.bigrams)
# IDF weights
IDF_vec = log(1 + N/colSums(test.bigrams > 0))
# TF-IDF weights:
# use sweep to multiply the columns (margin = 2) by the IDF weights
TFIDF_mat = sweep(TF_mat, MARGIN=2, STATS=IDF_vec, FUN="*")  

# PCA on the TF-IDF weights
pca = prcomp(TFIDF_mat, scale=TRUE)
pve = summary(pca)$importance[3,]
plot(pve)  # not much of an elbow

Xtest = pca$x[,1:1000]
ytest = {test.documents$author}


```




```{r}
library(randomForest)
finrf = randomForest(x = X, y = y, ntree=500)
```

```{r}
pred = predict(finrf, newx = Xtest, newy = ytest)
```

```{r}
comp = data.frame(cbind(as.character(pred), as.character(ytest)))
names(comp) = c('predicted', 'actual')
comp$correct = comp$predicted == comp$actual
```

```{r}
cat('Test set accuracy:', length(which(comp$correct))/2500)
```

```{r}
source("http://pcwww.liv.ac.uk/~william/R/crosstab.r")
tab = crosstab(comp, row.vars = "predicted", col.vars = "actual", type = "f")
write.csv(tab$crosstab, 'tab.csv')
tab = read.csv('tab.csv', header = TRUE, row.names = 1)
```

```{r}
for (i in 1:50){
  cat(as.character(tab[i,i]/.5),'%', ' accuracy predicting ', names(tab)[i], '\n', sep = '')
}
```

Report:

Data Pipeline: The Reuters C50 corpus consists of text documents from 50 authors, for our purposes split into a training and test set of 2500 documents each (50 per author per training/test set). I first read these text files into python and performed several preprocessing steps. 

First, I removed all non-alphanumeric characters and converted all text to lowercase, then used the package "nltk" to tokenize each document. I then removed all English stopwords and and performed stemming. Using pandas, I collect all of these preprocessed texts into a training and test dataframe, with each row containing three columns: the text, the author, and a category for if the document was part of the training or test set.

During this process, I also pulled information on the most common bigrams for each author. For every author, I pulled their documents from the training set and recorded their 100 most frequent bigrams. Considering duplicate bigrams between authors, this produced a list of 3165 bigrams to use an indicator of authorship. With these bigrams, I created two data frames, which counted the number of times that each of these bigrams appeared in each of our 5000 documents. For clarity: in these data frames each row represented a document, each of the 3165 columns represented the number of times a particuliar bigram appeared in the document.

Moving over to R, I read in each of these dataframes. For each of the test and training bigram dataframes of counts, I then added a small amount of noise (.001) to each observation. I then created a TF-IDF matrix for these bigrams, and ran a principal components analysis. Using the first 1000 principal components, I created a random forest model for the set of training documents. I then used this model to predict authorship for the test set with an accuracy of over 80%

